---
title: "Feature Engeneering / Text Processing"
author: "Petro Tolochko"
date: ""
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Preparation

### Required Packages

We first need to install the packages required for further analysis.

```{r, echo=FALSE}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
```

```{r, message=FALSE, results='hide'}

install.packages("tm")                            # probably already installed
install.packages("tidyverse")                     # probably already installed
install.packages("ggthemes") # for nicer graphics # new
install.packages("ggrepel")
install.packages("cowplot")
```

Note: you only need to install the packages once.

We then need load the packages in our environment:

```{r, message=FALSE, results='hide'}
library(tm)
library(tidyverse)
library(openNLP)
library(SnowballC)
library(ggthemes)
library(ggrepel)
```

Check if your working directory is the correct one:

```{r}
getwd()
setwd("~/Desktop/CEU_text_as_data/day_2/") # this is an example, paste your path here
```


Ok, now read in the file `federalist.csv` and inspect:

```{r}
federalist <- read_csv("federalist.csv")

federalist

names(federalist)
```

Let's see how a text looks like:

```{r}
federalist$text[1] # explain this!
```

Amazing, let's continue.


### Basic Pre-processing

```{r}

clean_federalist <- federalist %>%
  mutate(                           # the mutate() function is part of dplyr package / allows to change stuff within the dataframe easily
    text   = str_to_lower(text),                # turn all letters to lowercase
    text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
    text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
    man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
    by     = str_count(text, "\\Wby "),         # same
    upon   = str_count(text, "\\Wupon ")        # same
  ) %>%
  rowwise() %>%                                 # make future functions work rowwise
  mutate(
    length = length(str_split(text, " ")[[1]])  # calculate the length of the text (in words)
  )

```

How does it look like now?

```{r}
clean_federalist$text[1]
```

Nice! Did the word counting work?

```{r}
clean_federalist %>%
  select(man, by, upon)
```

Looks like it!
Now we're ready for some real analysis.

# Federalist Analysis [Simplified from (Grimmer at al., 2022)]

### Multinomial Model!

We'll do something else later, I promise.


Let's calculate the word rates for Hamilton, Madison, and Jay. Call them $\mu_{h}$, $\mu_{m}$, and $\mu_{j}$:

```{r}

mu_madison <- clean_federalist %>% 
  filter(author == "MADISON") %>%
  select(man, by, upon) %>%
  colSums()

mu_hamilton <- clean_federalist %>% 
  filter(author == "HAMILTON") %>%
  select(man, by, upon) %>%
  colSums()

mu_jay <- clean_federalist %>% 
  filter(author == "JAY") %>%
  select(man, by, upon) %>%
  colSums()

```

Check them:

```{r}
mu_madison; mu_hamilton; mu_jay
```

Now, calculate their probabilities and call them $\hat{\mu}_{h}$, $\hat{\mu}_{m}$, and $\hat{\mu}_{j}$:

```{r}
mu_hat_hamilton <- mu_hamilton / sum(mu_hamilton)
mu_hat_madison <- mu_madison / sum(mu_madison)
mu_hat_jay <- mu_jay / sum(mu_jay)
```

These are their word models! We're almost there...
Now, the Federalist paper ***#49*** is one of the disputed ones. First, let's see it:

```{r}
clean_federalist$text[49]
clean_federalist[49, ]
```



```{r}
### Disputed ###
mu_disputed <- clean_federalist[49, ] %>%
  select(man, by, upon)

mu_disputed
```

Calculating probabilities...
There's a built-in `dmultinom` function!
You can check what it's doing by typing `?dmultinom` in your console.

```{r}

p_disputed_hamilton <- dmultinom(mu_disputed,
                                 prob = mu_hat_hamilton)
p_disputed_madison  <- dmultinom(mu_disputed,
                                 prob = mu_hat_madison)
p_disputed_jay      <- dmultinom(mu_disputed,
                                 prob = mu_hat_jay)

```

And done!

```{r}
p_disputed_hamilton; p_disputed_madison; p_disputed_jay
```

As ***HOMEWORK*** (**Not graded**): try all the words (not just the ones we used here) and see if you get better or worse results. For additional ***no points*** you can also play around with *Laplace smoothing* -- i.e., adding a small number to the model to eliminate 0 probability words (e.g., add `1` to all counts before calculating probability).


# Vector Space Model

Let's look at the Federalist papers as vectors. Just the selected words for now (so, in the `man`, `by`, `upon` vector space).
We already have them as vector representations:



```{r}
mu_madison; mu_hamilton; mu_jay
```


We can try to visualize them. A bit difficult to do 3D (although possible), so let's try to visualize them in 2D.

First, we need to play around with data a bit, for visualization.

```{r}

vector_visualizations <- rbind(mu_madison, mu_hamilton, mu_jay, mu_disputed)

vector_visualizations$author = c("Madison", "Hamilton", "Jay", "Disputed")


```

Lets remove the `man` dimension for the visualization:

```{r}
vector_visualizations_no_man <- vector_visualizations %>%
  select(-man)

vector_visualizations_no_man
```

```{r}

vector_visualizations_no_man %>%
  ggplot() +
  geom_segment(aes(x = 0, y = 0, xend = by, yend = upon, color = factor(author)),
               arrow = arrow(length = unit(0.2,"cm")),
               size = 1) +
  theme_tufte() +
  theme(
    # legend.position = "none",
    
    axis.title.x = element_text(size = 18),
    axis.title.y = element_text(size = 18),
    axis.text.x = element_text(size = 18),
    axis.text.y = element_text(size = 18)
  ) +
  xlab("By") +
  ylab("Upon") +
  labs(color = 'Author')


```

Ok, not very helpful, let's try removing `by` dimension...

```{r}
vector_visualizations_no_by <- vector_visualizations %>%
  select(-by)

vector_visualizations_no_by %>%
  ggplot() +
  geom_segment(aes(x = 0, y = 0, xend = man, yend = upon, color = factor(author)),
               arrow = arrow(length = unit(0.2,"cm")),
               size = 1) +
  theme_tufte() +
  theme(
    # legend.position = "none",
    
    axis.title.x = element_text(size = 18),
    axis.title.y = element_text(size = 18),
    axis.text.x = element_text(size = 18),
    axis.text.y = element_text(size = 18)
  ) +
  xlab("Man") +
  ylab("Upon") +
  labs(color = 'Author')
```


Let's increase the vector magnitude for the `Disputed` vector. And maybe the `Jay` vector. Just to see it better:

```{r}
vector_visualizations_no_by[4, 1:2] <- vector_visualizations_no_by[4, 1:2] * 100
vector_visualizations_no_by[3, 1:2] <- vector_visualizations_no_by[3, 1:2] * 100
vector_visualizations_no_by %>%
  ggplot() +
  geom_segment(aes(x = 0, y = 0, xend = man, yend = upon, color = factor(author)),
               arrow = arrow(length = unit(0.2,"cm")),
               size = 1) +
  theme_tufte() +
  theme(
    # legend.position = "none",
    
    axis.title.x = element_text(size = 18),
    axis.title.y = element_text(size = 18),
    axis.text.x = element_text(size = 18),
    axis.text.y = element_text(size = 18)
  ) +
  xlab("Man") +
  ylab("Upon") +
  labs(color = 'Author')
```

So, just visually (on the dimensions `upon` and `man`), the `Disputed` paper has the smallest angle compared to `Madison`.

## Cosine similarity

Cosine similarity is ***!!! SURPRISE SURPRISE !!!*** a **similarity** measure between two vectors.

It is defined as:

$$
cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| ||\mathbf{B}||}
$$

Where $\mathbf{A} \cdot \mathbf{B} = A_1 \times B_1 + A_2 \times B_2 ...$ is the dot product of vectors, `%*%` operator in `r`. And $||A||$ is the **magnitude** (or, **Eucledian Norm**) of the vector -- $\sqrt{\Sigma^b_{i = 1}A^2_i}$.



$$
cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| ||\mathbf{B}||} = \frac{\Sigma^b_{i=1}A_iB_i}{\sqrt{\Sigma^b_{i = 1}A^2_i}\sqrt{\Sigma^b_{i = 1}B^2_i}}
$$

We can write the function:

```{r}
cosine_sim <- function(A, B) {
  numerator   <- A %*% B
  denominator <- sqrt(sum(A^2)) * sqrt(sum(B^2))
  
  similiarity <- numerator / denominator
  return(similiarity)
}
```

Let's get the data for analysis:

```{r}
vectors <- vector_visualizations[, 1:3] %>% as.matrix()
```

And now, let's calculate the similarity between them.

```{r}
similarity_matrix <- matrix(
  nrow = 4, ncol = 4
)

for (i in 1:nrow(similarity_matrix)) {
  for (j in 1:ncol(similarity_matrix)) {
    similarity_matrix[i, j] = cosine_sim(vectors[i, ], vectors[j, ])
  }
}

rownames(similarity_matrix) <- vector_visualizations$author
colnames(similarity_matrix) <- vector_visualizations$author

similarity_matrix

```

We don't have tons of data, so everything is very similar to everything else. But, it still provides evidence to the hypothesis, that the `Disputed` paper was written by `Madison`. Note that the highest similarity is with `Jay`, but 1) it was never hypothesized, and 2) the vector for `Jay` is very sparse, so difficult to make any meaningful measurements. 


Once again, for ***NO ADDITIONAL POINTS*** calculate the similarity between `Madison`, `Hamilton` and all other disputed papers (one by one). We can check which ones are disputed:

```{r}
clean_federalist %>%
  filter(author == "HAMILTON OR MADISON")
```


# Stop words and other preprocessing effects

What are the most used words in the federalist papers?

```{r}
dtm_federalist <- DocumentTermMatrix(clean_federalist$text,
                                     control = list(removePunctuation = TRUE,
                                         stopwords = FALSE))
dtm_federalist <- dtm_federalist %>% as.matrix()

dim(dtm_federalist)

most_frequent <- dtm_federalist %>% colSums()


most_frequent_df <- most_frequent %>% as.list() %>% as_tibble() %>%
  pivot_longer(everything())

most_frequent_df %>% arrange(desc(value))

```

```{r}

most_frequent_df %>%
  mutate(label = ifelse(value > 1200, name, NA)) %>%
  ggplot() +
  geom_point(aes(reorder(name, -value),  value)) +
  geom_text_repel(aes(reorder(name, -value),  value, label = label),
                  max.overlaps = 20) +
  theme_tufte() +
  ylab("Frequency") +
  theme(
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank()
  )

```

Zipf's Law (for many types of data studied in the physical and social sciences, the rank-frequency distribution is an inverse relation):

```{r}


most_frequent_df %>%
  arrange(desc(value)) %>%
  mutate(rank = 1:nrow(.)) %>%
  ggplot(aes(rank, value)) +
  geom_line() +
  ylab("Frequency") +
  xlab("Rank") +
  scale_x_log10() +
  scale_y_log10() +
  theme_tufte()

```



Not amazing.

What if we remove the "basic" stop-words?

```{r}
dtm_federalist <- DocumentTermMatrix(clean_federalist$text,
                                     control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))
dtm_federalist <- dtm_federalist %>% as.matrix()

dim(dtm_federalist)

most_frequent <- dtm_federalist %>% colSums()

most_frequent_df <- most_frequent %>% as.list() %>% as_tibble() %>%
  pivot_longer(everything())

most_frequent_df %>% arrange(desc(value))
```
Already looking a little better!

```{r}
most_frequent_df %>%
  mutate(label = ifelse(value > 500, name, NA)) %>%
  ggplot() +
  geom_point(aes(reorder(name, -value),  value)) +
  geom_text_repel(aes(reorder(name, -value),  value, label = label),
                  max.overlaps = 20) +
  theme_tufte() +
  ylab("Frequency") +
  theme(
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank()

  )

```


```{r}

most_frequent_df %>%
  arrange(desc(value)) %>%
  mutate(rank = 1:nrow(.)) %>%
  ggplot(aes(rank, value)) +
  geom_line() +
  ylab("Frequency") +
  xlab("Rank") +
  scale_x_log10() +
  scale_y_log10() +
  theme_tufte()
```

# TFIDF

$$
W_{ij} \times log\frac{N}{n_j}
$$

Super easy, actually. There's already a function implemented in the `tm` package. It's called `weightTfIdf`, and we should pass it to the control param of the `DocumentTermMatrix` function:

```{r}
dtm_federalist <- DocumentTermMatrix(clean_federalist$text,
                                     control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

dtm_federalist_tfidf <- weightTfIdf(dtm_federalist)


dtm_federalist_tfidf <- dtm_federalist_tfidf %>% as.matrix()

dim(dtm_federalist_tfidf)

most_frequent <- dtm_federalist_tfidf %>% colSums()

most_frequent_df <- most_frequent %>% as.list() %>% as_tibble() %>%
  pivot_longer(everything())

most_frequent_df %>% arrange(desc(value))
```

Not bad... Let's plot this:

```{r}
most_frequent_df %>%
  mutate(label = ifelse(value > .107, name, NA)) %>%
  ggplot() +
  geom_point(aes(reorder(name, -value),  value)) +
  geom_text_repel(aes(reorder(name, -value),  value, label = label),
                  max.overlaps = 20) +
  theme_tufte() +
  ylab("tf-idf") +
  theme(
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks = element_blank()

  )

```


# Log Odds

First, let's see if `tfidf` can be a good discriminant function between `Hamilton` and `Madison`?

```{r}

dtm_federalist <- DocumentTermMatrix(clean_federalist$text,
                                     control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

dtm_federalist_tfidf <- weightTfIdf(dtm_federalist)

hamilton_ids <- which(clean_federalist$author == "HAMILTON")
madison_ids <- which(clean_federalist$author == "MADISON")

most_frequent_hamilton <- dtm_federalist_tfidf[hamilton_ids, ] %>%
  as.matrix() %>% colSums()
most_frequent_madison <- dtm_federalist_tfidf[madison_ids, ] %>%
  as.matrix() %>% colSums()



most_frequent_df_hamilton <- most_frequent_hamilton %>%
  as.list() %>%
  as_tibble() %>%
  pivot_longer(everything()) %>% 
  mutate(author = "HAMILTON")


most_frequent_df_madison <- most_frequent_madison %>%
  as.list() %>%
  as_tibble() %>%
  pivot_longer(everything()) %>% 
  mutate(author = "MADISON")

most_frequent_df <- bind_rows(most_frequent_df_hamilton,
                              most_frequent_df_madison)

hamilton_plot <- most_frequent_df %>%
  filter(author == "HAMILTON") %>%
  top_n(20, value) %>%
  ggplot() +
  geom_bar(aes(reorder(name, value), value),
           stat = "identity",
           fill = "steelblue") +
  coord_flip() +
  ylab("ti-idf") +
  theme_tufte() +
  theme(axis.title.y = element_blank())

madison_plot <- most_frequent_df %>%
  filter(author == "MADISON") %>%
  top_n(20, value) %>%
  ggplot() +
  geom_bar(aes(reorder(name, value), value),
           stat = "identity",
           fill = "indianred4") +
  coord_flip() +
  ylab("ti-idf") +
  theme_tufte() +
  theme(axis.title.y = element_blank())


cowplot::plot_grid(hamilton_plot, madison_plot,
                   ncol = 2)


```

Let's try to calculate log odds.

$$
logO^i_w = log\frac{f^i_w}{1 - f^i_w}
$$

```{r}
dtm_federalist <- DocumentTermMatrix(clean_federalist$text,
                                     control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))

hamilton_ids <- which(clean_federalist$author == "HAMILTON")
madison_ids <- which(clean_federalist$author == "MADISON")

dtm_hamilton <- dtm_federalist[hamilton_ids, ] %>%
  as.matrix() %>% colSums()
freq_hamilton <- (dtm_hamilton + 1) / sum(dtm_hamilton)  # laplace smoothing

dtm_madison <- dtm_federalist[madison_ids, ] %>%
  as.matrix() %>% colSums()

freq_madison <- (dtm_madison + 1) / sum(dtm_madison)  # laplace smoothing


log_odds <- function(x) {
  log(x / (1 - x))
}

log_odds_ham <- log_odds(freq_hamilton)
log_odds_mad <- log_odds(freq_madison)


```

Now, log odds ratio:

$$
log\frac{O^i_w}{O^j_w} = log\frac{f^i_w}{1 - f^i_w}/\frac{f^j_w}{1 - f^j_w} = log\frac{f^i_w}{1 - f^i_w} - log\frac{f^j_w}{1 - f^j_w}
$$
```{r}
log_odds_ratio <- log_odds_ham - log_odds_mad



log_odds_ratio <- log_odds_ratio %>%
  as.list() %>%
  as_tibble() %>%
  pivot_longer(everything())

### Add word frequency ###

general_frequency <- dtm_federalist %>% as.matrix() %>% colSums() %>%
  as.list() %>%
  as_tibble() %>%
  pivot_longer(everything()) %>%
  mutate(freq = value) %>%
  select(-value)

log_odds_ratio <- log_odds_ratio %>%
  left_join(general_frequency)

log_odds_ratio %>%
  mutate(label = ifelse(value > 2 | value < -2, name, NA)) %>%
  ggplot() +
  geom_point(aes(freq, value, color = value)) +
  geom_text_repel(aes(freq, value, label = label),
                  max.overlaps = 50) +
  ylab("log odds ratio") +
  theme_tufte() +
  theme(legend.position = "none")


hamilton_plot <- log_odds_ratio %>%
  top_n(20, value) %>%
  ggplot() +
  geom_bar(aes(reorder(name, value), value),
           stat = "identity",
           fill = "steelblue") +
  coord_flip() +
  ylab("log odds ratio") +
  theme_tufte() +
  theme(axis.title.y = element_blank())

madison_plot <- log_odds_ratio %>%
  top_n(20, -value) %>%
  ggplot() +
  geom_bar(aes(reorder(name, -value), value),
           stat = "identity",
           fill = "indianred4") +
  coord_flip() +
  ylab("log odds ratio") +
  theme_tufte() +
  theme(axis.title.y = element_blank())


cowplot::plot_grid(hamilton_plot, madison_plot,
                   ncol = 2)

```



There are many problems with this method. Please refer to **Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict** by **Monroe**, **Colaresi** & **Quinn** (2009) for a very cool overview and improvement on the method.