---
title: "Text Representation"
author: "Petro Tolochko"
date: ""
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Preparation

### Required Packages

## WOW HELLO

We first need to install the packages required for further analysis.

```{r, echo=FALSE}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
```

```{r, message=FALSE, results='hide'}

# install.packages("tm")
# install.packages("tidyverse")

```

Note: you only need to install the packages once.

We then need load the packages in our environment:

```{r, message=FALSE, results='hide'}
library(tm)
require(tidyverse)

```


# Text Representation

Lets create the sample texts:


```{r, message=FALSE, results='hide'}
texts <- c("John loves icecream",
           "John loves oranges",
           "Marry hates icecream")
```

The `texts` object is what is know as a corpus -- a collection of texts. Our corpus size is 3 (3 different texts). You can check the length of the the corpus (or any vector in r) like this:

```{r}
length(texts)
```

### Document Term Matrix

We now need to convert the corpus in a bag-of-words text representation. Specifically, we want something like this:



$$
X = \begin{bmatrix}
1 &1  &... \\ 
 0& 2  &  ...  \\ 
 ...&...  &  .. 
\end{bmatrix}
$$



We can write our out function to do this, for example:

```{r}

simple_function <- function(anything) {
  anything + 1
}
simple_function_2 <- function() {
  print("hello, world")
}




create_dtm <- function(texts) {
  
  texts <- str_to_lower(texts)
  all_words <- str_split(texts, " ") %>% unlist()
  unique_terms <- str_split(texts, " ") %>% unlist() %>% unique()
  
  n <- length(texts)
  k <- length(unique_terms)
  x <- matrix(nrow = n, ncol = k, 0)
  
  for (i in 1:nrow(x)) {
    current_text <- str_split(texts[i], " ")[[1]]
    for (j in 1:ncol(x)) {
      for (y in 1:length(current_text)) {
        if (unique_terms[j] %in% current_text[y]) {
          x[i, j] = x[i, j] + 1
        } 
      }
    }
  }
  
  rownames(x) <- paste0("document_", 1:3)
  colnames(x) <- unique_terms
  
  return(x)
}
```

```{r}

create_dtm(texts)

```

Or, alternatively, we can use one of the many packages that already implement this functionality. For example, the package `tm` has the function `DocumentTermMatrix` to do exactly this:

```{r}

my_dtm <- DocumentTermMatrix(texts)
my_dtm

```

Although, as you can see, it is not exactly the best visual representation. We can look at a more normal representation using `inspect` function and convert this to a normal `matrix` form using `as.matrix` function.


```{r}

inspect(my_dtm)

my_dtm <- as.matrix(my_dtm)

my_dtm

```

### Task 1

Create a `Document Term Matrix` of the following texts:

```{r}
Text_1 = "banana  banana banana banana chocolate"
Text_3 = "chocolate chocolate chocolate banana"
Text_2 = "banana banana"

my_new_vector <- c(Text_1, Text_2, Text_3)

create_dtm(my_new_vector)


```

# Multinomial Model of Texts

The Multinomial Model of Texts is one of the simplest statistical text models that is used throughout many more sophisticated methods.

Remember, the Multinomial Distribution is a multivariate generalization of the Binomial Distribution and has the form:

$$
P_n(x_1, x_2,...x_n) = \frac{N!}{x_1!, x_2!...x_n!}p_1^{x_1}\times p_2^{x_2}...\times p_n^{x_n}
$$

We will now attempt to use this model to predict which data-generating process the texts come from.

First, we need to generate the `Document Term Matrix` of the following corpus:

```{r}
Text_1 = "banana  banana banana banana chocolate"
Text_2 = "banana banana"
Text_3 = "chocolate chocolate chocolate banana fudge"
Text_4 = "icecream icecream fudge icecream"
Text_5 = "fudge fudge fudge"
Text_6 = "icecream icecream fudge fudge"
Text_7 = "icecream  fudge fudge"
Text_8 = "chocolate chocolate banana banana"
```

Next, we assume that texts 1 through 3 are generated by `John`, while texts 4 though 6 are generated by `Mary`. `Text_7` and `Text_8` are of unknown origin. And we must determine who said what!

Create a `Document Term matrix` and call it `banana_data`:

```{r, echo=FALSE}

banana_corpus <- c(Text_1, Text_2, Text_3, Text_4, Text_5, Text_6,
                   Text_7, Text_8)
dtm <- DocumentTermMatrix(banana_corpus)
banana_data <- as.matrix(dtm) 

```

Next, we need to calculate `John` and `Mary` word rates (how often do they use certain words).

We do this by summing together all document rates that belong to `John` and to `Mary`, respectively.

In order to do so, you would need to subset the `Document Term Matrix` with the `[]` function, like so:

```{r}
banana_data[1:3, ]
```

This selects the first through third row for the matrix and all of the columns. Therefore, we have just selected the documents that `John` generated. We need to save it as an object:

```{r}
john_docs <- banana_data[1:3, ]
```

Now, we can use the `colSums` function to sum up the values across `columns`:

```{r}
john_rates <- colSums(john_docs)
```

Let's see what we have:

```{r}
john_rates
```

Finally, we want to calculate the relative rates (probabilities) of word use. We can do this by dividing the raw rates we have by the total number of words:

```{r}


total_john <- sum(john_rates)
probs_john <- john_rates / total_john
probs_john
```

Now, do the same for `Mary`. Remember, `Mary` generated texts 4 through 6.

```{r, echo=FALSE}
mary_docs <- banana_data[4:6, ]
mary_rates <- colSums(mary_docs)
total_mary <- sum(mary_rates)
probs_mary <- mary_rates / total_mary

```

You should have something that looks like this:

```{r}
probs_mary
```

# Now we have our Language Models

Next, we need to determine the rates of the unknown texts! Remember, the texts are `7` and `8`.

```{r}
unknown_1 <- banana_data[7, ]
unknown_2 <- banana_data[8, ]

```

Let's check:

```{r}
unknown_1
unknown_2
```

We can finally start calculating!

First, let's determine the probability of both of these texts belonging to `John`.

These are `John`'s probabilities:

```{r}
probs_john
```

And these are rates of the `Unknown document 1`:

```{r}
unknown_1
```

We need to estimate, according to the Multinomial Distribution, the following equation:

$$
P(b=0, c=0, f=2, i=1) = \frac{3!}{0!0!2!1!}0.58^{0}\times 0.33^{0} \times 0.08^2 \times 0^1
$$

Let's deal with the 

$$\frac{3!}{0!0!2!1!}$$

part first. The `!` symbol means `factorial`, basically

$$n! = x \times (n - 1) \times (n - 2) \times ... \times 3 \times 2 \times 1$$

so, 

$$5! = 5 \times 4 \times 3 \times 2 \times 1 = 120$$.

And $0! = 1$ by definition.

You can use the built in function `factorial`:

```{r}
factorial(5)
```

This thing:

$$\frac{3!}{0!0!2!1!}$$

thus becomes:

```{r}
factorial(3) / (factorial(0) * factorial(0) * factorial(2) * factorial(1))
```

Let's save it to an object:

```{r}
first_part <- factorial(3) / (factorial(0) * factorial(0) * factorial(2) * factorial(1))
```

Now, let's do this part:

$$
0.58^{0}\times 0.33^{0} \times 0.08^2 \times 0^1
$$

This is fairly straightforward:

```{r}
0.58^0 * 0.33^0 * 0.08^2 * 0^1
```

If you are paying attention, you kinda know where this is going...
Let's save the second part as well:

```{r}
second_part <- 0.58^0 * 0.33^0 * 0.08^2 * 0^1
```

And now, just multiply them:

```{r}
first_part * second_part

```

That's the probability that the first `Unknown text` was generated by `John`...

Let's do the second `Unknown text`:

$$
P(b=2, c=2, f=0, i=0) = \frac{4!}{2!2!0!0!}0.58^{2}\times 0.33^{2} \times 0.8^0 \times 0^0
$$

First part:

$$
\frac{4!}{2!2!0!0!}
$$

```{r}
first_part <- factorial(4) / (factorial(2) * factorial(2) * factorial(0) * factorial(0))
first_part
```

Second part:

$$
0.58^{2}\times 0.33^{2} \times 0.08^0 \times 0^0
$$

```{r}
second_part <- 0.58^2 * 0.33^2 * 0.08^0 * 0^0
```

(also, with a bit of *r-fu*, you can vectorize these operations, for example:)

```{r}

prod(probs_john ^ unknown_2)

# Rounding error, but close enough

second_part


```

So, what's the probability that the second text was generated by `John`?

```{r}
first_part * second_part
```

Not bad!

### Do the Mary model yourself :)